Metadata-Version: 2.4
Name: oxidizedvision
Version: 1.0.1
Summary: Compile your vision models to Rust for ultra-fast inference.
Author-email: Avaya Aggarwal <aggarwal.avaya27@gmail.com>
License: MIT
Project-URL: Homepage, https://github.com/onepunchmonk/Oxidized-Vision
Project-URL: Bug Tracker, https://github.com/onepunchmonk/Oxidized-Vision/issues
Project-URL: Documentation, https://github.com/onepunchmonk/Oxidized-Vision/tree/main/docs
Keywords: machine-learning,pytorch,onnx,rust,inference,deployment
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch>=1.12
Requires-Dist: torchvision
Requires-Dist: onnx>=1.12
Requires-Dist: onnx-simplifier
Requires-Dist: onnxruntime>=1.12
Requires-Dist: numpy>=1.21
Requires-Dist: typer>=0.7
Requires-Dist: pydantic>=1.10
Requires-Dist: pyyaml>=6.0
Requires-Dist: rich>=12.0
Requires-Dist: psutil>=5.9
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: black>=23.0; extra == "dev"
Requires-Dist: isort>=5.12; extra == "dev"
Requires-Dist: ruff>=0.1; extra == "dev"
Requires-Dist: mypy>=1.0; extra == "dev"
Provides-Extra: quantization
Requires-Dist: onnxruntime-extensions; extra == "quantization"
Dynamic: license-file

# ğŸš€ OxidizedVision

**Compile your PyTorch models to Rust for ultra-fast, memory-safe inference.**

OxidizedVision is a production-grade toolkit that bridges the gap between Python-based model training and Rust-based deployment. It provides a seamless pipeline to **convert**, **optimize**, **validate**, **benchmark**, **profile**, and **package** your models â€” from a trained PyTorch `nn.Module` to a deployable Rust binary, REST API, or WebAssembly module.

---

## âœ¨ Key Features

| Feature | Description |
|---|---|
| ğŸ”„ **Model Conversion** | PyTorch â†’ TorchScript â†’ ONNX with a single command |
| âš¡ **Optimization** | ONNX graph simplification, constant folding, INT8/FP16 quantization |
| âœ… **Validation** | Numerical consistency checks (MAE, RMSE, Cosine Similarity) across formats |
| ğŸ“Š **Benchmarking** | Latency (avg, p50, p95, p99), throughput, and memory profiling |
| ğŸ”¬ **Profiling** | Parameter count, model size, per-layer breakdown |
| ğŸ“¦ **Packaging** | Auto-generate a deployable Rust crate (server or CLI) |
| ğŸŒ **Multi-Backend** | `tract` (pure Rust), `tch` (LibTorch), `tensorrt` (NVIDIA GPU) |
| ğŸ§© **WASM Support** | Run models in the browser via WebAssembly |
| ğŸ“‹ **Model Registry** | Track all converted models and their metadata locally |
| ğŸ¨ **Rich CLI** | Beautiful terminal output with progress indicators and tables |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Python Client (CLI)                   â”‚
â”‚  convert â”‚ validate â”‚ benchmark â”‚ optimize â”‚ profile    â”‚
â”‚  package â”‚ serve    â”‚ list      â”‚ info                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Generates
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Rust Runtimes                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ runner_tch   â”‚  â”‚ runner_tract â”‚  â”‚ runner_tensorrt â”‚ â”‚
â”‚  â”‚ (LibTorch)   â”‚  â”‚ (Pure Rust)  â”‚  â”‚ (GPU / TensorRT)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚   All implement Runner trait      â”‚           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              runner_core (Shared Trait)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Deploys to
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                â–¼                â–¼
   Native Binary    REST API Server    WASM Module
```

---

## âš¡ Quickstart

### 1. Install

```bash
pip install -e "./python_client"
```

### 2. Create a Config

```yaml
# config.yml
model:
  path: examples/example_unet/model.py
  class_name: UNet
  input_shape: [1, 3, 256, 256]

export:
  output_dir: out
  model_name: unet

validate:
  tolerance_mae: 1e-4
  tolerance_cos_sim: 0.999

benchmark:
  iters: 100
  device: cpu
```

### 3. Run the Pipeline

```bash
# Convert PyTorch â†’ TorchScript + ONNX
oxidizedvision convert config.yml

# Validate numerical consistency
oxidizedvision validate config.yml

# Optimize the ONNX model
oxidizedvision optimize out/unet.onnx --quantize int8

# Benchmark performance
oxidizedvision benchmark out/unet.pt --runners torchscript,tract

# Profile the model
oxidizedvision profile config.yml

# Package into a Rust crate
oxidizedvision package out/unet.onnx --runner tract --template server

# List registered models
oxidizedvision list
```

---

## ğŸ“– CLI Reference

| Command | Description | Example |
|---|---|---|
| `convert` | Convert PyTorch â†’ TorchScript + ONNX | `oxidizedvision convert config.yml` |
| `validate` | Check numerical consistency | `oxidizedvision validate config.yml --num-tests 5` |
| `benchmark` | Measure inference performance | `oxidizedvision benchmark out/model.pt --runners torchscript,tract` |
| `optimize` | Optimize an ONNX model | `oxidizedvision optimize out/model.onnx --quantize fp16` |
| `profile` | Analyze model parameters and layers | `oxidizedvision profile config.yml` |
| `package` | Generate deployable Rust crate | `oxidizedvision package out/model.onnx --template server` |
| `serve` | Start inference server | `oxidizedvision serve ./binary --port 8080` |
| `list` | List registered models | `oxidizedvision list` |
| `info` | Detailed model information | `oxidizedvision info unet` |

---

## ğŸ¦€ Rust Runtimes

### Shared Runner Trait

All backends implement a common `Runner` trait:

```rust
pub trait Runner: Send + Sync {
    fn from_config(config: &RunnerConfig) -> Result<Self> where Self: Sized;
    fn run(&self, input: &ArrayD<f32>) -> Result<ArrayD<f32>>;
    fn info(&self) -> ModelInfo;
}
```

### Available Backends

| Backend | Model Format | GPU | WASM | Dependencies |
|---|---|---|---|---|
| `runner_tract` | ONNX | âŒ | âœ… | None (pure Rust) |
| `runner_tch` | TorchScript | âœ… | âŒ | LibTorch |
| `runner_tensorrt` | ONNX â†’ Engine | âœ… | âŒ | TensorRT SDK |

---

## ğŸ—‚ï¸ Project Structure

```
Oxidized-Vision/
â”œâ”€â”€ python_client/             # Python CLI & pipeline
â”‚   â”œâ”€â”€ oxidizedvision/
â”‚   â”‚   â”œâ”€â”€ cli.py             # Typer CLI entry point
â”‚   â”‚   â”œâ”€â”€ config.py          # Pydantic config models
â”‚   â”‚   â”œâ”€â”€ convert.py         # Model conversion
â”‚   â”‚   â”œâ”€â”€ validate.py        # Numerical validation
â”‚   â”‚   â”œâ”€â”€ benchmark.py       # Performance measurement
â”‚   â”‚   â”œâ”€â”€ optimize.py        # ONNX optimization
â”‚   â”‚   â”œâ”€â”€ profile.py         # Model profiling
â”‚   â”‚   â””â”€â”€ registry.py        # Model registry
â”‚   â””â”€â”€ tests/                 # pytest test suite
â”œâ”€â”€ rust_runtime/              # Rust inference runtimes
â”‚   â”œâ”€â”€ crates/
â”‚   â”‚   â”œâ”€â”€ runner_core/       # Shared Runner trait
â”‚   â”‚   â”œâ”€â”€ runner_tch/        # LibTorch backend
â”‚   â”‚   â”œâ”€â”€ runner_tract/      # tract (ONNX) backend
â”‚   â”‚   â””â”€â”€ runner_tensorrt/   # TensorRT backend
â”‚   â””â”€â”€ examples/
â”‚       â”œâ”€â”€ image_server/      # Actix-web REST API server
â”‚       â”œâ”€â”€ denoiser_cli/      # Image denoising CLI
â”‚       â””â”€â”€ wasm_frontend/     # Browser inference demo
â”œâ”€â”€ tools/                     # Standalone scripts
â”œâ”€â”€ benchmarks/                # Benchmark infrastructure
â”œâ”€â”€ examples/                  # User-facing examples
â”‚   â””â”€â”€ example_unet/         # Complete UNet example
â”œâ”€â”€ docs/                      # Architecture docs
â””â”€â”€ .github/workflows/         # CI/CD
```

---

## ğŸ§ª Testing

```bash
# Python tests
pytest python_client/tests/ -v --cov=oxidizedvision

# Rust tests
cargo test --workspace
```

---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.
