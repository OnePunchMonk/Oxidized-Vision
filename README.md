# ğŸš€ OxidizedVision

[![CI](https://github.com/OnePunchMonk/Oxidized-Vision/actions/workflows/ci.yml/badge.svg)](https://github.com/OnePunchMonk/Oxidized-Vision/actions/workflows/ci.yml)
[![PyPI](https://img.shields.io/pypi/v/oxidizedvision)](https://pypi.org/project/oxidizedvision/)
[![Python](https://img.shields.io/pypi/pyversions/oxidizedvision)](https://pypi.org/project/oxidizedvision/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Compile your PyTorch models to Rust for ultra-fast, memory-safe inference.**

OxidizedVision is a production-grade toolkit that bridges the gap between Python-based model training and Rust-based deployment. It provides a seamless pipeline to **convert**, **optimize**, **validate**, **benchmark**, **profile**, and **package** your models â€” from a trained PyTorch `nn.Module` to a deployable Rust binary, REST API, or WebAssembly module.

---

## âœ¨ Key Features

| Feature | Description |
|---|---|
| ğŸ”„ **Model Conversion** | PyTorch â†’ TorchScript â†’ ONNX with a single command |
| âš¡ **Optimization** | ONNX graph simplification, constant folding, INT8/FP16 quantization |
| âœ… **Validation** | Numerical consistency checks (MAE, RMSE, Cosine Similarity) across formats |
| ğŸ“Š **Benchmarking** | Latency (avg, p50, p95, p99), throughput, and memory profiling |
| ğŸ”¬ **Profiling** | Parameter count, model size, per-layer breakdown |
| ğŸ“¦ **Packaging** | Auto-generate a deployable Rust crate (server or CLI) |
| ğŸŒ **Multi-Backend** | `tract` (pure Rust), `tch` (LibTorch), `tensorrt` (NVIDIA GPU) |
| ğŸ§© **WASM Support** | Run models in the browser via WebAssembly |
| ğŸ“‹ **Model Registry** | Track all converted models and their metadata locally |
| ğŸ¨ **Rich CLI** | Beautiful terminal output with progress indicators and tables |
| ğŸ”€ **Multi-Model Server** | Serve multiple models from a single Rust server instance |
| â±ï¸ **Dynamic Batching** | Configurable request batching for efficient inference |
| ğŸ“ **Structured Logging** | `tracing` (Rust) + Rich/JSON (Python) for full observability |
| ğŸ“ˆ **Metrics Endpoint** | `/metrics` for monitoring request counts and server health |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Python Client (CLI)                   â”‚
â”‚  convert â”‚ validate â”‚ benchmark â”‚ optimize â”‚ profile    â”‚
â”‚  package â”‚ serve    â”‚ list      â”‚ info                   â”‚
â”‚                                                         â”‚
â”‚  Global: --verbose  â”‚  --json-log                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Generates
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Rust Runtimes                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ runner_tch   â”‚  â”‚ runner_tract â”‚  â”‚ runner_tensorrt â”‚ â”‚
â”‚  â”‚ (LibTorch)   â”‚  â”‚ (Pure Rust)  â”‚  â”‚ (GPU / TensorRT)â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚   All implement Runner trait      â”‚           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              runner_core (Shared Trait)             â”‚ â”‚
â”‚  â”‚          + tracing structured logging              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ Deploys to
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                â–¼                â–¼
   Native Binary    REST API Server    WASM Module
                    (multi-model,
                     batching,
                     /metrics)
```

---

## âš¡ Quickstart

### 1. Install

```bash
# From PyPI
pip install oxidizedvision

# From source (development)
pip install -e "./python_client[dev]"
```

### 2. Create a Config

```yaml
# config.yml
model:
  path: examples/example_unet/model.py
  class_name: UNet
  input_shape: [1, 3, 256, 256]

export:
  output_dir: out
  model_name: unet

validate:
  tolerance_mae: 1e-4
  tolerance_cos_sim: 0.999

benchmark:
  iters: 100
  device: cpu
```

### 3. Run the Pipeline

```bash
# Convert PyTorch â†’ TorchScript + ONNX
oxidizedvision convert config.yml

# Validate numerical consistency
oxidizedvision validate config.yml

# Optimize the ONNX model
oxidizedvision optimize out/unet.onnx --quantize int8

# Benchmark performance
oxidizedvision benchmark out/unet.pt --runners torchscript,tract

# Profile the model
oxidizedvision profile config.yml

# Package into a Rust crate
oxidizedvision package out/unet.onnx --runner tract --template server

# List registered models
oxidizedvision list
```

### 4. Debug with Structured Logging

```bash
# Verbose mode (DEBUG level)
oxidizedvision --verbose convert config.yml

# JSON log output (for CI / log aggregation)
oxidizedvision --json-log convert config.yml
```

---

## ğŸ“– CLI Reference

| Command | Description | Example |
|---|---|---|
| `convert` | Convert PyTorch â†’ TorchScript + ONNX | `oxidizedvision convert config.yml` |
| `validate` | Check numerical consistency | `oxidizedvision validate config.yml --num-tests 5` |
| `benchmark` | Measure inference performance | `oxidizedvision benchmark out/model.pt --runners torchscript,tract` |
| `optimize` | Optimize an ONNX model | `oxidizedvision optimize out/model.onnx --quantize fp16` |
| `profile` | Analyze model parameters and layers | `oxidizedvision profile config.yml` |
| `package` | Generate deployable Rust crate | `oxidizedvision package out/model.onnx --template server` |
| `serve` | Start inference server | `oxidizedvision serve ./binary --port 8080` |
| `list` | List registered models | `oxidizedvision list` |
| `info` | Detailed model information | `oxidizedvision info unet` |

### Global Options

| Flag | Description |
|---|---|
| `--verbose` / `-v` | Enable DEBUG-level logging |
| `--json-log` | Emit logs as JSON lines (for CI / production) |

---

## ğŸ¦€ Rust Runtimes

### Shared Runner Trait

All backends implement a common `Runner` trait:

```rust
pub trait Runner: Send + Sync {
    fn from_config(config: &RunnerConfig) -> Result<Self> where Self: Sized;
    fn run(&self, input: &ArrayD<f32>) -> Result<ArrayD<f32>>;
    fn info(&self) -> ModelInfo;
}
```

### Available Backends

| Backend | Model Format | GPU | WASM | Dependencies |
|---|---|---|---|---|
| `runner_tract` | ONNX | âŒ | âœ… | None (pure Rust) |
| `runner_tch` | TorchScript | âœ… | âŒ | LibTorch |
| `runner_tensorrt` | ONNX â†’ Engine | âœ… | âŒ | TensorRT SDK |

---

## ğŸ–¥ï¸ Inference Server

The built-in `image_server` example provides a production-ready REST API:

```bash
# Single model
cargo run -p image_server -- --model model.onnx --port 8080

# Multi-model (serve multiple models simultaneously)
cargo run -p image_server -- \
  --model segmenter=models/seg.onnx \
  --model classifier=models/cls.onnx \
  --port 8080

# With dynamic batching
cargo run -p image_server -- \
  --model model.onnx \
  --max-batch-size 8 \
  --max-wait-ms 50

# JSON structured logs
cargo run -p image_server -- --model model.onnx --log-format json
```

### Endpoints

| Method | Path | Description |
|---|---|---|
| `POST` | `/predict` | Inference on the default model |
| `POST` | `/predict/{model_name}` | Inference on a named model |
| `GET` | `/health` | Health check with per-model status |
| `GET` | `/metrics` | Request counts, error counts, batch status |
| `GET` | `/models` | List all loaded models |

---

## ğŸ—‚ï¸ Project Structure

```
Oxidized-Vision/
â”œâ”€â”€ python_client/             # Python CLI & pipeline
â”‚   â”œâ”€â”€ oxidizedvision/
â”‚   â”‚   â”œâ”€â”€ cli.py             # Typer CLI entry point
â”‚   â”‚   â”œâ”€â”€ config.py          # Pydantic config models
â”‚   â”‚   â”œâ”€â”€ convert.py         # Model conversion
â”‚   â”‚   â”œâ”€â”€ validate.py        # Numerical validation
â”‚   â”‚   â”œâ”€â”€ benchmark.py       # Performance measurement
â”‚   â”‚   â”œâ”€â”€ optimize.py        # ONNX optimization
â”‚   â”‚   â”œâ”€â”€ profile.py         # Model profiling
â”‚   â”‚   â”œâ”€â”€ registry.py        # Model registry
â”‚   â”‚   â””â”€â”€ logging.py         # Structured logging (Rich / JSON)
â”‚   â””â”€â”€ tests/                 # pytest test suite
â”œâ”€â”€ rust_runtime/              # Rust inference runtimes
â”‚   â”œâ”€â”€ crates/
â”‚   â”‚   â”œâ”€â”€ runner_core/       # Shared Runner trait + tracing
â”‚   â”‚   â”œâ”€â”€ runner_tch/        # LibTorch backend
â”‚   â”‚   â”œâ”€â”€ runner_tract/      # tract (ONNX) backend
â”‚   â”‚   â””â”€â”€ runner_tensorrt/   # TensorRT backend
â”‚   â””â”€â”€ examples/
â”‚       â”œâ”€â”€ image_server/      # Multi-model REST API with batching
â”‚       â”œâ”€â”€ denoiser_cli/      # Image denoising CLI
â”‚       â””â”€â”€ wasm_frontend/     # Browser inference demo
â”œâ”€â”€ tools/                     # Standalone scripts
â”œâ”€â”€ benchmarks/                # Benchmark infrastructure
â”œâ”€â”€ examples/                  # User-facing examples
â”‚   â””â”€â”€ example_unet/         # Complete UNet example
â”œâ”€â”€ docs/                      # Architecture docs
â””â”€â”€ .github/workflows/         # CI/CD + PyPI auto-deploy
```

---

## ğŸ§ª Testing

```bash
# Python tests
pytest python_client/tests/ -v --cov=oxidizedvision

# Rust tests
cargo test --workspace
```

### Pre-commit Hooks

```bash
pip install pre-commit
pre-commit install
pre-commit run --all-files
```

---

## ğŸ“¦ Publishing to PyPI

Releases are automatically published to PyPI when a GitHub Release is created with a `v*` tag (e.g., `v1.0.2`). See [`.github/workflows/publish.yml`](.github/workflows/publish.yml) for details.

To publish manually:

```bash
pip install build twine
python -m build
twine upload dist/*
```

---

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for development setup, testing instructions, and PR guidelines.

---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.
